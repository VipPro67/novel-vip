#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os, re, time, html, sys, urllib.parse, mimetypes, argparse, threading
import requests
from bs4 import BeautifulSoup
from ebooklib import epub
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

# ===================== argparse (verbosity) =====================
def parse_args():
    p = argparse.ArgumentParser(description="Universal TOC ‚Üí EPUB crawler (+optional backend upload)")
    p.add_argument("--verbose", action="store_true", help="Print extra debug info")
    return p.parse_args()

ARGS = parse_args()

def vprint(*a, **k):
    if ARGS.verbose:
        print(*a, **k)

# ===================== HTTP session (retry + pool) =====================
def build_session():
    s = requests.Session()
    s.headers.update({"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"})
    retry = Retry(
        total=5, connect=5, read=5,
        backoff_factor=0.8,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods={"GET", "HEAD"},
        raise_on_status=False
    )
    adapter = HTTPAdapter(max_retries=retry, pool_connections=100, pool_maxsize=100)
    s.mount("http://", adapter)
    s.mount("https://", adapter)
    return s

# ===================== Helpers =====================
ALLOWED_INLINE = {"b", "i", "u", "br"}
AD_WORDS = ["ads", "banner", "qc", "quang-cao", "promo", "incontent-ad", "sponsor"]

def is_ad_tag(tag):
    if not tag or not hasattr(tag, "attrs"):
        return False
    for attr in ("id", "class"):
        v = tag.get(attr)
        if not v:
            continue
        text = " ".join(v) if isinstance(v, list) else str(v)
        text = text.lower()
        if any(kw in text for kw in AD_WORDS):
            return True
    return False

def clean_paragraph_tag(tag):
    # gi·ªØ b/i/u/br, unwrap ph·∫ßn c√≤n l·∫°i
    for child in tag.find_all(True):
        if child.name not in ALLOWED_INLINE:
            child.unwrap()
    s = str(tag)
    s = re.sub(r"\s+", " ", s)
    return s.strip()

def absolutize(base, href):
    return urllib.parse.urljoin(base, href)

def to_slug(s):
    s = s.strip().lower()
    s = re.sub(r"[^a-z0-9]+", "-", s)
    return s.strip("-")

# ===================== Content extract (generic) =====================
def extract_title_generic(soup, chap_num=None, novel_title=None):
    """
    Tr√≠ch ti√™u ƒë·ªÅ ch∆∞∆°ng an to√†n, ∆∞u ti√™n ch√≠nh x√°c v·ªã tr√≠.
    """
    title_text = None

    # 1Ô∏è‚É£ Truyenchuth.info ‚Äî h3 ch·ª©a icon c√¢y b√∫t (fa-pencil-square-o)
    h3_tag = None
    header = soup.select_one(".chapter-header") or soup.select_one(".chapter-title")
    if header:
        for h3 in header.find_all("h3"):
            # ch·ªâ l·∫•y h3 c√≥ ch·ªØ 'Ch∆∞∆°ng' ho·∫∑c c√≥ icon fa-pencil-square-o
            if h3.find("i", class_=re.compile("fa-pencil-square")) or re.search(r"Ch∆∞∆°ng\s*\d+", h3.get_text()):
                h3_tag = h3
                break
    if h3_tag:
        title_text = h3_tag.get_text(strip=True)

    # 2Ô∏è‚É£ fallback ph·ªï bi·∫øn (h1, h2)
    if not title_text:
        for tag in soup.select("h1, h2, h3"):
            text = tag.get_text(strip=True)
            if re.search(r"Ch∆∞∆°ng\s*\d+", text):
                title_text = text
                break

    # 3Ô∏è‚É£ fallback th·∫ª <title> ho·∫∑c meta og:title
    if not title_text:
        meta = soup.select_one("meta[property='og:title']")
        if meta and meta.get("content"):
            title_text = meta["content"]
        else:
            tag = soup.select_one("title")
            if tag:
                title_text = tag.get_text(strip=True)

    # 4Ô∏è‚É£ fallback cu·ªëi
    if not title_text:
        title_text = f"Ch∆∞∆°ng {chap_num}" if chap_num else "Kh√¥ng r√µ ti√™u ƒë·ªÅ"

    # 5Ô∏è‚É£ lo·∫°i b·ªè ph·∫ßn ch·ª©a t√™n truy·ªán n·∫øu d√≠nh
    if novel_title and title_text.lower().startswith(novel_title.lower()):
        parts = title_text.split("-", 1)
        if len(parts) > 1:
            title_text = parts[-1].strip()

    # 6Ô∏è‚É£ clean nh·∫π
    title_text = re.sub(r"\s+", " ", title_text).strip()

    vprint(f"[title] Extracted: {title_text}")
    return title_text


def extract_content_generic(soup):
    # ph·ªï bi·∫øn cho nhi·ªÅu theme
    content = (
        soup.select_one("#chapter-content")
        or soup.select_one(".chapter-content")
        or soup.select_one("article .entry-content")
        or soup.select_one("article")
        or soup.select_one("div.entry-content")
        or soup.select_one("div.reading-content")
    )
    if not content:
        return None
    for x in content(["script", "style"]):
        x.decompose()
    for tag in content.find_all(True):
        if is_ad_tag(tag):
            tag.decompose()

    parts = []
    for node in content.find_all(["p", "div"], recursive=True):
        txt = clean_paragraph_tag(node)
        if txt and re.search(r"\w", txt, flags=re.UNICODE):
            if not txt.startswith("<p"):
                txt = f"<p>{txt}</p>"
            parts.append(txt)

    parts = [
        p for p in parts
        if not re.search(r"(ƒë·ªçc.*nhanh.*nh·∫•t|truy(?:e|√™)n.*nhanh|theo d√µi.*page|b·∫£n quy·ªÅn|like.*share)", p, re.I)
    ]
    if not parts:
        return None
    return "\n".join(parts)

# ===================== Content extract (truyenchuth.info) =====================
def _pick_best_selector(soup, candidates):
    best_sel, best_len, best_node = None, 0, None
    for sel in candidates:
        node = soup.select_one(sel)
        if not node:
            continue
        txt = node.get_text(strip=True) if node else ""
        ln = len(txt or "")
        if ln > best_len:
            best_len, best_sel, best_node = ln, sel, node
    return best_sel, best_node

def extract_content_truyenchuth(soup):
    """
    Extract n·ªôi dung ch∆∞∆°ng t·ª´ truyenchuth.info
    DOM th·∫≠t: <div id="content" class="w3-justify chapter-content detailcontent">
    """
    # Ch·ªçn v√πng n·ªôi dung ch√≠nh
    content = soup.select_one("div#content.chapter-content") or soup.select_one("#content")
    if not content:
        vprint("[truyenchuth] ‚ùå Kh√¥ng t√¨m th·∫•y div#content")
        return None
    vprint("[truyenchuth] ‚úÖ T√¨m th·∫•y div#content.chapter-content")

    # Lo·∫°i b·ªè c√°c th·∫ª kh√¥ng c·∫ßn
    for x in content(["script", "style"]):
        x.decompose()
    for tag in content.find_all(True):
        if is_ad_tag(tag):
            tag.decompose()

    # Gh√©p l·∫°i ƒëo·∫°n vƒÉn: trang n√†y ch·ªâ c√≥ <br> ngƒÉn d√≤ng, kh√¥ng c√≥ <p>
    html_parts = []
    buffer = []
    for elem in content.children:
        if getattr(elem, "name", None) == "br":
            # Khi g·∫∑p <br><br> => k·∫øt th√∫c m·ªôt ƒëo·∫°n
            if buffer:
                text = "".join(buffer).strip()
                if text:
                    html_parts.append(f"<p>{html.escape(text)}</p>")
                buffer = []
        elif isinstance(elem, str):
            # text node
            t = elem.strip()
            if t:
                buffer.append(t + " ")
        else:
            # n·∫øu c√≥ tag kh√°c nh∆∞ <b><i> trong text
            t = elem.get_text(" ", strip=True)
            if t:
                buffer.append(t + " ")

    # Ph·∫ßn c√≤n l·∫°i sau v√≤ng l·∫∑p
    if buffer:
        text = "".join(buffer).strip()
        if text:
            html_parts.append(f"<p>{html.escape(text)}</p>")

    # X√≥a d√≤ng qu·∫£ng c√°o v√¥ nghƒ©a
    html_parts = [
        p for p in html_parts
        if not re.search(r"(truyenchuth|ƒë·ªçc.*nhanh.*nh·∫•t|like.*share|theo d√µi.*page)", p, re.I)
    ]

    return "\n".join(html_parts) if html_parts else None

# ===================== TOC adapters =====================
CH_PATTERN = re.compile(r"/chuong-?(\d+)[^/]*", re.I)

def _parse_chapter_number_from_href(href):
    m = CH_PATTERN.search(href)
    return int(m.group(1)) if m else None

def list_chapters_truyenfull_vision(session, base_url):
    print("üîé L·∫•y danh s√°ch ch∆∞∆°ng (truyenfull.vision)‚Ä¶")
    results, seen = [], set()

    if "/trang-" not in base_url:
        base_url = base_url.rstrip("/") + "/trang-1/"

    next_url = base_url
    while next_url:
        print(f"  ‚Ä¢ ƒêang duy·ªát trang danh s√°ch: {next_url}")
        res = session.get(next_url, timeout=20)
        if res.status_code != 200:
            print("    ‚Ü≥ l·ªói t·∫£i trang, d·ª´ng.")
            break
        soup = BeautifulSoup(res.text, "html.parser")
        list_container = soup.select_one("#list-chapter") or soup.select_one(".list-chapter") or soup
        found_in_page = 0
        for a in list_container.find_all("a", href=True):
            n = _parse_chapter_number_from_href(a["href"])
            if n is None:
                continue
            abs_url = absolutize(next_url, a["href"])
            if abs_url in seen:
                continue
            seen.add(abs_url)
            title = a.get_text(strip=True) or f"Ch∆∞∆°ng {n}"
            results.append((n, abs_url, title))
            found_in_page += 1
        print(f"    ‚Ü≥ t√¨m ƒë∆∞·ª£c {found_in_page} ch∆∞∆°ng ·ªü trang n√†y. T·ªïng t·∫°m th·ªùi: {len(results)}")

        nav_next = soup.find("a", attrs={"rel": "next"}) or soup.find("a", string=re.compile(r"Trang\s*sau|Sau|Next|¬ª", re.I))
        if nav_next and nav_next.get("href"):
            next_url = absolutize(next_url, nav_next["href"])
        else:
            m = re.search(r"/trang-(\d+)/", next_url)
            if m:
                nxt = int(m.group(1)) + 1
                probe = re.sub(r"/trang-\d+/", f"/trang-{nxt}/", next_url)
                head = session.head(probe)
                next_url = probe if head.status_code == 200 else None
            else:
                next_url = None

    results.sort(key=lambda x: x[0])
    print(f"‚úÖ T·ªïng ch∆∞∆°ng l·∫•y ƒë∆∞·ª£c: {len(results)}")
    return results

def list_chapters_truyenchuth_info(session, base_url, max_pages=200, max_empty_pages=1):
    print("üîé L·∫•y danh s√°ch ch∆∞∆°ng (truyenchuth.info)‚Ä¶")
    results, seen = [], set()

    parsed = urllib.parse.urlsplit(base_url)
    query = urllib.parse.parse_qs(parsed.query)
    p = int(query.get("p", ["1"])[0])
    empty_in_a_row = 0
    pages = 0

    while pages < max_pages:
        page_url = urllib.parse.urlunsplit((parsed.scheme, parsed.netloc, parsed.path, f"p={p}", ""))
        print(f"  ‚Ä¢ ƒêang duy·ªát trang danh s√°ch: {page_url}")
        res = session.get(page_url, timeout=20)
        if res.status_code != 200:
            print("    ‚Ü≥ l·ªói t·∫£i trang, d·ª´ng.")
            break

        soup = BeautifulSoup(res.text, "html.parser")

        nav_next = soup.find("a", attrs={"rel": "next"}) or soup.find("a", string=re.compile(r"Trang\s*sau|Sau|Next|¬ª", re.I))

        found_in_page = 0
        # T√¨m container ch·ª©a danh s√°ch ch∆∞∆°ng
        list_container = soup.select_one("div#divtab.list-chapter")

        # Ch·ªâ l·∫•y <a> trong container n√†y
        for a in list_container.find_all("a", href=True):
            m = CH_PATTERN.search(a["href"])
            if not m:
                continue
            try:
                n = int(m.group(1))
            except ValueError:
                continue

            abs_url = absolutize(page_url, a["href"])
            if abs_url in seen:
                continue
            seen.add(abs_url)

            title = a.get_text(strip=True) or f"Ch∆∞∆°ng {n}"
            results.append((n, abs_url, title))
            found_in_page += 1

        print(f"    ‚Ü≥ t√¨m ƒë∆∞·ª£c {found_in_page} ch∆∞∆°ng ·ªü trang n√†y. T·ªïng t·∫°m th·ªùi: {len(results)}")

        pages += 1
        if found_in_page == 0:
            empty_in_a_row += 1
            if empty_in_a_row >= max_empty_pages:
                print("    ‚Ü≥ g·∫∑p trang r·ªóng, d·ª´ng qu√©t.")
                break
        else:
            empty_in_a_row = 0

        if nav_next and nav_next.get("href"):
            next_url = absolutize(page_url, nav_next["href"])
            try:
                nqs = urllib.parse.urlsplit(next_url).query
                np = int(urllib.parse.parse_qs(nqs).get("p", ["0"])[0])
                p = np if np > 0 else p + 1
            except Exception:
                p += 1
        else:
            p += 1

    results.sort(key=lambda x: x[0])
    print(f"‚úÖ T·ªïng ch∆∞∆°ng l·∫•y ƒë∆∞·ª£c: {len(results)}")
    return results

def list_chapters_truyenchu_net(session, base_url):
    print("üîé L·∫•y danh s√°ch ch∆∞∆°ng (truyenchu.net)‚Ä¶")
    results, seen = [], set()
    parsed = urllib.parse.urlsplit(base_url)
    q = urllib.parse.parse_qs(parsed.query)

    def build_page_url(pg):
        query = dict(q)
        query["page"] = [str(pg)]
        query_str = urllib.parse.urlencode({k: v[0] for k, v in query.items()})
        path = parsed.path if parsed.path.endswith("/") else parsed.path + "/"
        return urllib.parse.urlunsplit((parsed.scheme, parsed.netloc, path, query_str, ""))

    if "page" not in q:
        base_url = build_page_url(1)

    next_url = base_url
    while next_url:
        print(f"  ‚Ä¢ ƒêang duy·ªát trang danh s√°ch: {next_url}")
        res = session.get(next_url, timeout=20)
        if res.status_code != 200:
            print("    ‚Ü≥ l·ªói t·∫£i trang, d·ª´ng.")
            break
        soup = BeautifulSoup(res.text, "html.parser")
        list_container = soup.select_one("#list-chapter") or soup.select_one(".list-chapter") or soup
        found_in_page = 0
        for a in list_container.find_all("a", href=True):
            n = _parse_chapter_number_from_href(a["href"])
            if n is None:
                continue
            abs_url = absolutize(next_url, a["href"])
            if abs_url in seen:
                continue
            seen.add(abs_url)
            title = a.get_text(strip=True) or f"Ch∆∞∆°ng {n}"
            results.append((n, abs_url, title))
            found_in_page += 1
        print(f"    ‚Ü≥ t√¨m ƒë∆∞·ª£c {found_in_page} ch∆∞∆°ng ·ªü trang n√†y. T·ªïng t·∫°m th·ªùi: {len(results)}")

        nav_next = soup.find("a", attrs={"rel": "next"}) or soup.find("a", string=re.compile(r"Trang\s*sau|Sau|Next|¬ª", re.I))
        if nav_next and nav_next.get("href"):
            next_url = absolutize(next_url, nav_next["href"])
        else:
            m = re.search(r"[?&]page=(\d+)", next_url)
            if m:
                nxt = int(m.group(1)) + 1
                probe = re.sub(r"([?&]page=)\d+", rf"\g<1>{nxt}", next_url)
                head = session.head(probe)
                next_url = probe if head.status_code == 200 else None
            else:
                next_url = None

    results.sort(key=lambda x: x[0])
    print(f"‚úÖ T·ªïng ch∆∞∆°ng l·∫•y ƒë∆∞·ª£c: {len(results)}")
    return results

# ===================== Router for TOC =====================
def list_chapters_from_base(session, base_url):
    host = urllib.parse.urlsplit(base_url).netloc
    if "truyenfull.vision" in host:
        return list_chapters_truyenfull_vision(session, base_url)
    if "truyenchuth.info" in host:
        return list_chapters_truyenchuth_info(session, base_url)
    if "truyenchu.net" in host:
        return list_chapters_truyenchu_net(session, base_url)

    print("‚ÑπÔ∏è Domain ch∆∞a c√≥ adapter ri√™ng ‚Äî d√πng fallback qu√©t link /chuong-<n>‚Ä¶")
    results, seen = [], set()
    next_url = base_url
    while next_url:
        print(f"  ‚Ä¢ ƒêang duy·ªát trang danh s√°ch (fallback): {next_url}")
        r = session.get(next_url, timeout=20)
        if r.status_code != 200:
            print("    ‚Ü≥ l·ªói t·∫£i trang, d·ª´ng.")
            break
        soup = BeautifulSoup(r.text, "html.parser")
        found_in_page = 0
        for a in soup.find_all("a", href=True):
            n = _parse_chapter_number_from_href(a["href"])
            if n is None:
                continue
            abs_url = absolutize(next_url, a["href"])
            if abs_url in seen:
                continue
            seen.add(abs_url)
            title = a.get_text(strip=True) or f"Ch∆∞∆°ng {n}"
            results.append((n, abs_url, title))
            found_in_page += 1
        print(f"    ‚Ü≥ t√¨m ƒë∆∞·ª£c {found_in_page} ch∆∞∆°ng ·ªü trang n√†y. T·ªïng t·∫°m th·ªùi: {len(results)}")
        nav_next = soup.find("a", attrs={"rel": "next"}) or soup.find("a", string=re.compile(r"Next|Sau|Trang\s*sau|¬ª", re.I))
        next_url = absolutize(next_url, nav_next["href"]) if (nav_next and nav_next.get("href")) else None
    results.sort(key=lambda x: x[0])
    print(f"‚úÖ T·ªïng ch∆∞∆°ng l·∫•y ƒë∆∞·ª£c: {len(results)}")
    return results

# ===================== Crawl chapter (with progress) =====================
_progress_lock = threading.Lock()
_progress_done = 0
_progress_total = 0
_start_time = 0.0

def _print_progress(prefix="Crawl"):
    with _progress_lock:
        done = _progress_done
        total = _progress_total
        elapsed = max(0.001, time.time() - _start_time)
        rate = done / elapsed
        print(f"  {prefix}: {done}/{total} done | {rate:.2f} chap/s | elapsed {elapsed:.1f}s", end="\r", flush=True)

def fetch_one_chapter(session, chap_num, url, novel_title):
    global _progress_done
    try:
        vprint(f"‚Üí GET {url}")
        res = session.get(url, timeout=20)
        if res.status_code != 200:
            vprint(f"  ‚Ü≥ HTTP {res.status_code} (skip)")
            with _progress_lock:
                _progress_done += 1
            _print_progress()
            return chap_num, None, None

        soup = BeautifulSoup(res.text, "html.parser")
        title = extract_title_generic(soup, chap_num, novel_title)

        host = urllib.parse.urlsplit(url).netloc
        if "truyenchuth.info" in host:
            body = extract_content_truyenchuth(soup) or extract_content_generic(soup)
        else:
            body = extract_content_generic(soup)

        if not body:
            vprint("  ‚Ü≥ empty body (skip)")
            with _progress_lock:
                _progress_done += 1
            _print_progress()
            return chap_num, title, None

        html_content = f"<h2>{html.escape(title)}</h2>\n{body}"
        vprint(f"  ‚Ü≥ OK: {title}")
        with _progress_lock:
            _progress_done += 1
        _print_progress()
        return chap_num, title, html_content

    except Exception as e:
        vprint(f"  ‚Ü≥ EXCEPTION: {e}")
        with _progress_lock:
            _progress_done += 1
        _print_progress()
        return chap_num, None, None

# ===================== EPUB builder (robust) =====================
def create_epub(novel_title, author, ordered_chapters, output_path):
    print("\nüß± ƒêang build EPUB‚Ä¶")
    book = epub.EpubBook()
    book.set_identifier("novel-" + to_slug(novel_title))
    book.set_title(novel_title)
    book.set_language("vi")
    book.add_author(author or "Kh√¥ng r√µ")

    items, empty_cnt = [], 0
    for chap_num, title, html_content in ordered_chapters:
        display_title = title or f"Ch∆∞∆°ng {chap_num} (L·ªói)"

        final_body = html_content if (html_content and str(html_content).strip()) else (
            f"<h2>{html.escape(display_title)}</h2><p><i>Kh√¥ng l·∫•y ƒë∆∞·ª£c n·ªôi dung ch∆∞∆°ng n√†y.</i></p>"
        )
        if final_body == html_content:
            # ok
            pass
        else:
            empty_cnt += 1

        xhtml = (
            "<?xml version='1.0' encoding='utf-8'?>\n"
            "<!DOCTYPE html>\n"
            f"<html xmlns='http://www.w3.org/1999/xhtml' xml:lang='vi'>"
            f"<head><meta charset='utf-8'/><title>{html.escape(display_title)}</title></head>"
            f"<body>{final_body}</body></html>"
        )

        item = epub.EpubHtml(title=display_title, file_name=f"chap_{chap_num}.xhtml", lang="vi")
        item.set_content(xhtml.encode("utf-8"))  # tr√°nh empty/encoding edge-cases
        book.add_item(item)
        items.append(item)

    if not items:
        # b·∫£o hi·ªÉm: v·∫´n t·∫°o 1 file h·ª£p l·ªá
        fallback = epub.EpubHtml(title="EMPTY", file_name="chap_0.xhtml", lang="vi")
        fallback.set_content(b"<?xml version='1.0' encoding='utf-8'?><!DOCTYPE html><html xmlns='http://www.w3.org/1999/xhtml' xml:lang='vi'><head><meta charset='utf-8'/><title>EMPTY</title></head><body><h2>EMPTY</h2><p>No content.</p></body></html>")
        book.add_item(fallback)
        items.append(fallback)

    book.toc = tuple(items)
    book.add_item(epub.EpubNcx())
    book.add_item(epub.EpubNav())
    book.spine = ["nav"] + items

    epub.write_epub(output_path, book)
    print(f"‚úÖ EPUB created: {output_path}")
    if empty_cnt:
        print(f"‚ö†Ô∏è C√≥ {empty_cnt} ch∆∞∆°ng placeholder (kh√¥ng l·∫•y ƒë∆∞·ª£c n·ªôi dung).")

# ===================== Optional: upload to backend =====================
def upload_to_backend(epub_path, api_base, slug, token, status=None):
    """
    POST {api_base}/api/novels/import-epub?slug=<slug>&status=<status>
    Form-Data: epub=@file
    Header: Authorization: Bearer <token>
    """
    url = api_base.rstrip("/") + "/api/novels/import-epub"
    params = {"slug": slug}
    if status:
        params["status"] = status
    headers = {"Authorization": f"Bearer {token}"} if token else {}
    files = {"epub": (os.path.basename(epub_path), open(epub_path, "rb"), mimetypes.guess_type(epub_path)[0] or "application/epub+zip")}
    print(f"üì§ Uploading to {url}?slug={slug}{'&status='+status if status else ''}")
    try:
        r = requests.post(url, headers=headers, params=params, files=files, timeout=90)
        print("   ‚Ü≥ Response:", r.status_code)
        print("   ‚Ü≥ Body   :", (r.text[:500] + ("‚Ä¶" if len(r.text) > 500 else "")))
        r.raise_for_status()
        print("‚úÖ Import queued successfully.")
    except Exception as e:
        print("‚ùå Import failed:", e)

# ===================== Controller =====================
def run_pipeline():
    print("=== üìö Universal TOC ‚Üí EPUB (3-in-1) ===")
    novel_title =  "C·ª≠a h√†ng s·ªßng th√∫ c·ª≠a h√†ng"
    toc_url = "https://truyenchuth.info/truyen-sieu-than-sung-thu-cua-hang"
    start_chap = 1
    end_chap = 1426
    author = "C·ªï Hi"

    session = build_session()
    print("‚è≥ ƒêang l·∫•y danh s√°ch ch∆∞∆°ng t·ª´ trang m·ª•c l·ª•c‚Ä¶")
    links = list_chapters_from_base(session, toc_url)
    if not links:
        print("‚ùå Kh√¥ng t√¨m th·∫•y danh s√°ch ch∆∞∆°ng t·ª´ URL m·ª•c l·ª•c.")
        sys.exit(1)

    wanted = [(n, u, t) for (n, u, t) in links if start_chap <= n <= end_chap]
    print(f"üìë T·ªïng ch∆∞∆°ng c√≥ trong kho·∫£ng [{start_chap}..{end_chap}]: {len(wanted)}")
    if not wanted:
        print("‚ùå Kh√¥ng c√≥ ch∆∞∆°ng n√†o trong kho·∫£ng y√™u c·∫ßu.")
        sys.exit(1)

    # Sample inspect ch∆∞∆°ng ƒë·∫ßu ƒë·ªÉ x√°c nh·∫≠n selector ·ªïn
    probe_n, probe_u, _ = wanted[0]
    print(f"üß™ Ki·ªÉm tra selector b·∫±ng ch∆∞∆°ng ƒë·∫ßu: {probe_u}")
    _tmp = fetch_one_chapter(session, probe_n, probe_u, novel_title)
    if not _tmp or not _tmp[2]:
        print("‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng tr√≠ch ƒë∆∞·ª£c n·ªôi dung ·ªü ch∆∞∆°ng m·∫´u. V·∫´n ti·∫øp t·ª•c crawl, nh∆∞ng kh·∫£ nƒÉng FAIL cao n·∫øu selector sai.")

    total = len(wanted)
    threaded = total > 100
    workers = min(20, max(5, total // 5)) if threaded else 1
    print(f"üöÄ B·∫Øt ƒë·∫ßu crawl: {total} ch∆∞∆°ng | mode={'Threaded' if threaded else 'Single'} | workers={workers}")

    global _progress_done, _progress_total, _start_time
    _progress_done = 0
    _progress_total = total
    _start_time = time.time()

    results = []
    t0 = time.time()
    if threaded:
        with ThreadPoolExecutor(max_workers=workers) as ex:
            futs = [ex.submit(fetch_one_chapter, session, n, u, novel_title) for (n, u, _) in wanted]
            for fut in as_completed(futs):
                results.append(fut.result())
    else:
        for (n, u, _) in wanted:
            results.append(fetch_one_chapter(session, n, u, novel_title))
            time.sleep(0.1)

    print()
    elapsed_crawl = time.time() - t0
    ok_cnt = sum(1 for _, _, h in results if h)
    fail_cnt = total - ok_cnt
    print(f"üìä Crawl xong: OK {ok_cnt} | FAIL {fail_cnt} | th·ªùi gian {elapsed_crawl:.1f}s (~{ok_cnt/max(0.001,elapsed_crawl):.2f} chap/s)")

    results.sort(key=lambda x: x[0])
    out = os.path.abspath(f"{to_slug(novel_title)}.epub")
    create_epub(novel_title, author, results, out)

    # print("\n‚Äî B∆∞·ªõc 4: G·ª≠i l√™n backend import-epub ‚Äî")
    # yn = input("Upload l√™n backend? (y/N): ").strip().lower()
    # if yn == "y":
    #     api_base = input("API base (vd: https://api.novel-vip.xyz): ").strip()
    #     slug = input("Slug (vd: {}): ".format(to_slug(novel_title))).strip() or to_slug(novel_title)
    #     token = input("Bearer token (paste ho·∫∑c b·ªè tr·ªëng): ").strip()
    #     status = input("Status (vd: ONGOING/DRAFT ‚Äì b·ªè tr·ªëng n·∫øu kh√¥ng): ").strip() or None
    #     upload_to_backend(out, api_base, slug, token, status)
    # else:
    #     print("‚úÖ D·ª´ng ·ªü b∆∞·ªõc 3 (EPUB ƒë√£ t·∫°o).")

if __name__ == "__main__":
    run_pipeline()
